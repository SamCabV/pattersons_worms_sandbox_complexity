# Question:  What is your understanding of the experiment the team is replicating?  What question does it answer?  How clear is the team's explanation?
	
	Our understanding of the experiment the team is replicating is pretty clear from the abstract: it is using cellular automata in an agent based model to simulate driver behavior in traffic. The question this experiment is trying to answer is ‘what is the relationship between the number of cars and the number of collisions over a period of time.’ The question was not as clear as what the experiment was composed of as I was only able to figure it out from the result interpretation section of the writing.

# Methodology: Do you understand the methodology?  Does it make sense for the question?  Are there limitations you see that the team did not address?
	
	While it was clear agent based modeling was used in this experiment, the actual method of implementation was not as clear: such as how the circle was generated, how far each car moved or how the ‘anti-collision’ behavior was encoded into each agent. Some more explanation on the implementation itself would be nice. However it appears from the results that their experiment/model makes sense for the question they are trying to answer. They certainly addressed that there was no method for validation which is fair in this type of modelling, we are not sure what the team will do if they are not able to implement merging with multiple lanes.

# Results: Do you understand what the results are (not yet considering their interpretation)?  If they are presented graphically, are the visualizations effective?  Do all figures have labels on the axes and captions?
	
	We understand the graph and images of the results however in the results section a lot of ‘expect to see’ is discussed and not what is seen in the current results relative to that. The graphical representations are clear but from them and the description it is not exactly clear what information is being synthesized from the results. Not all of the figures have labels/captions and there are typos on the axes. 

# Interpretation: Does the draft report interpret the results as an answer to the motivating question?  Does the argument hold water?
	
	Since it seems like the implementation is not yet complete, there is no clear argument being made in the results interpretation.

# Replication: Are the results in the report consistent with the results from the original paper?  If so, how did the authors demonstrate that consistency?  Is it quantitative or qualitative?
	
	It appears that the results of the report may be consistent on a qualitative level with the original paper but no examples are given within the submitted paper.

# Extension: Does the report explain an extension to the original experiment clearly?  Can it answer an interesting question that the original experiment did not answer?
	
	The extension is explained somewhat vaguely, however it seems the effect of merging was not explored in the original paper. Perhaps explaining why this would change the behavior of the model would be a helpful guide?

# Progress: Is the team roughly where they should be at this point, with a replication that is substantially complete and an extension that is clearly defined and either complete or nearly so?
	
	The team seems to be a little behind but there may have been a rush to submit this assignment to reach this evaluatory stage.
